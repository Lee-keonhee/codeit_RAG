
'''
í…ìŠ¤íŠ¸ ì²­í‚¹ ë° ì „ì²˜ë¦¬
ì…ë ¥ : {
    "doc_id": "RFP_001",
    "text": "ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸...",
    "metadata": {...}
}

ì¶œë ¥ : [
    {
        "chunk_id": "RFP_001_chunk_0",
        "text": "ì²­í¬1 í…ìŠ¤íŠ¸...",
        "metadata": {
            "doc_id": "RFP_001",
            "ë°œì£¼ê¸°ê´€": "êµ­ë¯¼ì—°ê¸ˆê³µë‹¨",
            "ì‚¬ì—…ëª…": "ì´ëŸ¬ë‹ì‹œìŠ¤í…œ êµ¬ì¶•",
            "chunk_index": 0
        }
    },
    {
        "chunk_id": "RFP_001_chunk_1",
        "text": "ì²­í¬2 í…ìŠ¤íŠ¸...",
        "metadata": {...}
    }
]

ì£¼ìš” í•¨ìˆ˜:
def clean_text(text: str) -> str
def chunk_text(text: str, chunk_size: int, overlap: int) -> List[str]
def process_document(doc: Dict, config: Dict) -> List[Dict]
'''
import re
from typing import List, Dict
from langchain_text_splitters import RecursiveCharacterTextSplitter
from tqdm import tqdm

def clean_text(text: str) -> str:
    """
    í…ìŠ¤íŠ¸ ì •ì œ

    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸

    Returns:
        ì •ì œëœ í…ìŠ¤íŠ¸
    """
    # 1. null ë°”ì´íŠ¸ ì œê±°
    text = text.replace('\x00', '')

    # 2. ì œì–´ ë¬¸ì ì œê±° (ê°œí–‰, íƒ­ì€ ìœ ì§€)
    text = re.sub(r'[\x00-\x08\x0b-\x0c\x0e-\x1f\x7f-\x9f]', '', text)

    # 3. í•„ìš”í•œ ìœ ë‹ˆì½”ë“œ ë¬¸ìë§Œ ìœ ì§€
    # ASCII + í•œê¸€ + ë¡œë§ˆìˆ«ì + ì›ë¬¸ì + íŠ¹ìˆ˜ê¸°í˜¸
    # text = re.sub(r'[^\u0000-\u007F\uAC00-\uD7A3]+', '', text)
    text = re.sub(r'[^\u0000-\u007F\uAC00-\uD7A3\u2160-\u217F\u2460-\u24FF\u2022\u25A0-\u25FF]+', '', text)

    # 4. ê°œí–‰ ì •ë¦¬
    text = re.sub(r'\r\n', '\n', text)

    # 5. ê³µë°± ì •ë¦¬
    text = re.sub(r' +', ' ', text)
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r' +\n', '\n', text)
    text = re.sub(r' *\n *', '\n', text)

    # 6. ì—°ì† ê°œí–‰ ì •ë¦¬ (ìµœëŒ€ 3ê°œ)
    text = re.sub(r'\n{3,}', '\n\n\n', text)

    # 7. ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()

    return text


def chunk_text(text: str, chunk_size: int, chunk_overlap: int) -> List[str]:
    """
    í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 

    Args:
        text: ë¶„í• í•  í…ìŠ¤íŠ¸
        chunk_size: ì²­í¬ í¬ê¸°
        chunk_overlap: ì²­í¬ ê°„ ì¤‘ì²© í¬ê¸°

    Returns:
        ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["\n\n\n", "\n", ". ", " ", ""],
        length_function=len
    )

    chunks = splitter.split_text(text)
    return chunks


def process_document(document: Dict, config: Dict) -> List[Dict]:
    """
    ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ì—¬ ì²­í¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±

    Args:
        document: ë¬¸ì„œ ë”•ì…”ë„ˆë¦¬
            {
                'doc_id': str,
                'text': str,
                'metadata': dict
            }
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
            {
                'chunk_size': int,
                'chunk_overlap': int
            }

    Returns:
        ì²­í¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    # ì„¤ì • ì¶”ì¶œ
    chunk_size = config['chunk_size']
    chunk_overlap = config['chunk_overlap']

    # 1. í…ìŠ¤íŠ¸ ì •ì œ
    cleaned_text = clean_text(document['text'])

    # 2. í…ìŠ¤íŠ¸ ì²­í‚¹
    text_chunks = chunk_text(cleaned_text, chunk_size, chunk_overlap)

    # 3. ì²­í¬ ë”•ì…”ë„ˆë¦¬ ìƒì„±
    chunks = []
    for chunk_index, chunk_content in enumerate(text_chunks):
        chunk_dict = {
            'chunk_id': f"{document['doc_id']}_chunk_{chunk_index}",
            'text': chunk_content,
            'metadata': {
                **document['metadata'],  # ì›ë³¸ metadata ë³µì‚¬
                'doc_id': document['doc_id'],
                'chunk_index': chunk_index,
                'total_chunks': len(text_chunks)
            }
        }
        chunks.append(chunk_dict)
    return chunks

def process_all_documents(documents: List[Dict], config: Dict) -> List[Dict]:
    """
    ëª¨ë“  ë¬¸ì„œ ì²˜ë¦¬

    Args:
        documents: ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬

    Returns:
        ì „ì²´ ì²­í¬ ë¦¬ìŠ¤íŠ¸
    """
    all_chunks = []

    print(f"ğŸ“„ Processing {len(documents)} documents...")

    for doc in tqdm(documents, desc="Chunking"):
        try:
            doc_chunks = process_document(doc, config)
            all_chunks.extend(doc_chunks)
        except Exception as e:
            print(f"\nâŒ Error processing {doc.get('doc_id', 'unknown')}: {e}")
            continue

    print(f"âœ… ì´ {len(documents)}ê°œ ë¬¸ì„œ â†’ {len(all_chunks)}ê°œ ì²­í¬ ìƒì„±")

    # í†µê³„ ì¶œë ¥
    avg_chunks = len(all_chunks) / len(documents) if documents else 0
    print(f"ğŸ“Š ë¬¸ì„œë‹¹ í‰ê·  ì²­í¬ ìˆ˜: {avg_chunks:.1f}")

    return all_chunks


if __name__ == '__main__':
    from data_loader import load_documents

    # ì„¤ì •
    config = {
        'data_dir': '../data/raw/files',
        'metadata_path': '../data/raw/data_list.csv',
        'chunk_size': 1000,
        'chunk_overlap': 200,
    }

    # ë¬¸ì„œ ë¡œë“œ
    documents = load_documents(config['data_dir'], config['metadata_path'])

    # ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ â­
    all_chunks = process_all_documents(documents, config)

    # í™•ì¸
    print(f"\nì²« ë²ˆì§¸ ì²­í¬ í™•ì¸:")
    print(f"chunk_id: {all_chunks[0]['chunk_id']}")
    print(f"text ê¸¸ì´: {len(all_chunks[0]['text'])}")
    print(f"metadata: {all_chunks[0]['metadata']}")